>>>markdown
Welcome to the Groundhog Day data analysis demo in Malloy!

![groundhog day](https://akns-images.eonline.com/eol_images/Entire_Site/201611/rs_596x341-160201175140-Groundhog_Day_again.gif?fit=around%7C596:341&output-quality=90&crop=596:341;center,top)

The purpose of this demo is to:

1. Help me learn Malloy. I've done a fairly intense 6 months or so of SQL but am not a data engineer
   and mostly do "normal" software development (Java, Kotlin, C, Python, Lua...
   that sort of thing)

2. Highlight current sharp edges and areas of improvement for the Malloy team
   from the perspective of said newb.

3. Better understand the prophetic abilities of prognosticating groundhogs
   through the power of data.

Callout to [Data is
Plural](https://www.data-is-plural.com/archive/2023-02-08-edition/) for the
link to groundhog da<b>y</b>ta

## Assumptions
An accurate groundhog day prediction is whether the groundhog correctly
predicts that spring will NOT come within the next 6 weeks (because he "sees
his shadow" and retreats to his den).

To make things simple we will define "Early Spring" as:

\> Spring has sprung early when the maximum "daily average maximum temperature"
\> of Feb and March is >= 40

I can't say for certain this is how the groundhogs themselves define "Spring",
but it's good enough for our purpose. Future iterations should probably
refine this metric to each state and preferably interview the groundhogs
themselves.

## Data
Data is included in this repository in both the raw and generated form. To
re-generate the data from it's `raw/` form into it's `gen/` form run:

```
make gen
```

Raw data used in this demo can be downloaded from

* `raw_data/noaa.txt`: ncei.noaa.gov
  (`https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-norm-tmaxdv-v1.0.0-{date}`
  (note: the date changes)
  * This is a narly custom text format. See
    [`scripts/noaa.py`](./scripts/noaa.py) for how I converted the data to CSV
* `raw_data/groundhogs.json` copy/pasted from https://groundhog-day.com/api

The csv files are generated using the python scripts in `scripts/`

## Getting our Groundhog Data
In Malloy we use a `source` to pull data in and then `extend`, `rename`
and `dimension` (calculate) a few fields, as well as define a primary key
for joins.
>>>malloy
source: GroundhogsRaw is duckdb.table("./gen/groundhogs.csv") extend {
  primary_key: year_
  rename: id is slug, state is region
  -- null for fields like: "No Record.,1897-02-02"
  where: shadow != null
  dimension:
    year_ is year(early_spring),
    predict_early_spring is shadow = 0
}
>>>markdown
The next bit I would _like_ to do in a single pipeline like the following,
but I have to do it in multiple named steps instead:

```
source: ... extend { ... } -> { project: ... } -> { primary_key: }
```

(this is for the malloy devs) the things blocking this are:

1. Get only a certain set of fields for better viewing. Unforunately in the
   `extend` you have to `accept` all fields you use in a `dimension` meaning I
   need the intermediary `project` to fully filter them (I like my data to be
   neat when I print)

2. You don't seem to be able to do a pipeline of `source -> query -> source`,
   instead you need to name each intermediary stage.

3. You cannot define a primary key in a query (only a source)

Anyway, let's work around these issues and transform our data so that its
dimensions are better aligned with the kinds of questions we might ask it:

* `year_state` will be the primary key we join against our temperature data.

* `id` is the "unique name" of the groundhog that we will group by.

* `predict_early_spring` indicates whether the groundhog predicts spring will
  "come early"

>>>malloy
-- eliminate uninteresting columns
query: G1 is GroundhogsRaw -> {
   project:
    year_ is year(early_spring),
    state,
    year_state is concat(year_, ' ', state),
    predict_early_spring,
    name, id, `day` is early_spring
}

-- slap on a primary key
source: Groundhogs is G1 extend {
  -- you can't do `primary_key: (year_, state)`
  primary_key: year_state
}
>>>markdown
Before we continue, take a look at the current schema.

>>>malloy
run: Groundhogs -> {
  project: *
  top: 10
}

>>>markdown
## Get our Temperature data

Now that we have our groundhogs and their predictions, let's get some data to
join against them. As noted in [Data](#data) we are using data from
[ncei.noaa.gov](http://ncei.noaa.gov) transformed to CSV with a script.

\> Note: temp_max is the **average maximum temperature** across all days in the
\> month (I don't think it's the maximum maximum temperature), not that this
\> really matters much for our purposes.

(for the Malloy devs) the below query needs to be split into three parts because:

1. Malloy doesn't support "projecting" a value that was aggregated
   in that `extend` block
   (I can't do `is_early_spring is early_spring_temp_max >= 40`)

2. query doesn't support primary_key

Oh well, such is life with a new-ish language. **Just Do It:**

>>>malloy

source: T1 is duckdb.table('./gen/TempMax.csv') extend {
  accept:      `date`, state, max_temp
  rename:      temp_max is max_temp
  dimension:   SPRING_TEMP is 40
}

-- aggregate the stuff we renamed
query: T2 is T1 -> {
  where:     month(`date`) ? 2 to 3
  aggregate: early_spring_temp_max is max(temp_max)
  group_by:
    year_ is year(`date`)
    state
    SPRING_TEMP -- yuck, wish we had "real" constants
} -> {
  project:
    *,
    year_state is concat(year_, ' ', state)
    is_early_spring is early_spring_temp_max >= SPRING_TEMP
}

-- slap on a primary key
source: YearStateSpring is T2 extend {
  primary_key: year_state
}
>>>markdown
## Sidequest: get context

It would be nice to include some contextual information such as:

* How often a given state has early spring.

* A reference point for how difficult it is for a prophet (groundhog or not) to
  predict whether spring will come for a given place.

For the latter, Let's use an extremely simplistic measurement as the distance
(absolute difference) from the definition of spring.  We'll also compute the
maximum distance so we have something to normalize with.

Let's create some sources for that.

>>>malloy

query: PES0 is YearStateSpring -> {
  group_by: state
  aggregate:
    ratio_early_spring is sum(pick 1 when is_early_spring else 0) / count()
    state_avg_distance_spring is
      sum(abs(SPRING_TEMP - early_spring_temp_max)) / count()
    -- maximum distance across all states/years
    max_distance_spring is
      all(max(abs(SPRING_TEMP - early_spring_temp_max)))
} -> {
  project:
    -- The distance factor lets us put a weight on the prophetic
    -- vision of our shadow viewing groundpig.
    *, state_distance_factor is state_avg_distance_spring / max_distance_spring
}

-- slap on a primary key
source: AllYearsStateSpring is PES0 extend {
  primary_key: state
}

>>>markdown
## Putting it all together
Now that we have our data let's join it and perform the calculations we care
about

>>>malloy
query: GroundhogPredictions is Groundhogs -> {
  join_one: YearStateSpring with year_state
  join_one: AllYearsStateSpring with state
  project:
    *,
    -- accuracy: is it the most important?
    accurate is YearStateSpring.is_early_spring = predict_early_spring,
    YearStateSpring.is_early_spring,

    -- or is distance factor more important?
    AllYearsStateSpring.state_avg_distance_spring,
    AllYearsStateSpring.state_distance_factor,
    state_ratio_early_spring is AllYearsStateSpring.ratio_early_spring,
} -> {
  project: *
    -- Let's try to create an absolute measure of prophecy powers.
    -- b/4242: multiply by a thousand to boost our numbers
    prophetic_wisdom is 1000 * (pick 1 when accurate else 0)
                             / state_distance_factor
}

>>>markdown
Finally let's aggregate our primary result (`ratio_accuracy`, the most
important (?) metric for any prophetic rodent) as well as some reference
points.

Let's also compare against the state averages since that might turn up some
useful insights into shadow meterology.

>>>malloy
query: GroundhogAccuracy is GroundhogPredictions -> {
  group_by:
    id, name, state
    -- note: remains constant
    state_ratio_early_spring, state_distance_factor, state_avg_distance_spring
  aggregate:
    ratio_accuracy is avg(pick 1 when accurate else 0),
    ratio_predict_early_spring is
      sum(pick 1 when predict_early_spring else 0) / count()
    ratio_is_early_spring is
      sum(pick 1 when is_early_spring else 0) / count()
    num_predictions is count(),
    earliest_prediction is min(concat(year_)),
    prophetic_wisdom is sum(prophetic_wisdom) / count()
}

-- Make a view that is neat for displaying
query: RodentProphets is GroundhogAccuracy -> {
  project:
    name,

    # percent
    ratio_accuracy,

    -- b/4242: multiply by a thousand to boost our numbers
    -- prophetic_wisdom is 1000 * ratio_accuracy / state_distance_factor

    prophetic_wisdom
    state_avg_distance_spring

    # percent
    ratio_is_early_spring

    # percent
    state_ratio_early_spring

    # percent
    ratio_predict_early_spring

    state,
    earliest_prediction
    num_predictions
}

-- Order and display data to draw (false) conclusions
run: RodentProphets -> {
  project: *
  order_by:
    ratio_accuracy desc, ratio_is_early_spring desc
  where:
    num_predictions > 5
}
>>>markdown

Right away we can see that the top groundhog "General Beauregard Lee" is a
cheater: sure he's accurate 90% of the time, but it's also spring 100% of the
time in Georgia!

Prophet "Chuckles" is a significantly more interesting case. Not only is he
prophetic, but his state has undergone quite a lot of warming: it's lifetime
average of early spring is only 30%, but it's average since predictions started
(2008) is up to 73%.

There's plenty of rodents we could mock: The next three have only an 71% - 83%
accuracy in a state that has early spring 100% of the time. However, it's
important to remember they might have a different definition of "spring" then
we do.

But wait, let's see how things look if instead of accuracy we sort by the aptly
named **prophetic_wisdom**

>>>malloy
-- Order and display data to draw prophetic conclusions
run: RodentProphets -> {
  project:  *
  order_by: prophetic_wisdom desc
  where:    num_predictions > 5
}

>>>markdown
We can now see clearly that General Beauregard Lee is as untinteresting as he
seemed, with only 1,576 points of wisdom.  Being highly accurate in a state
that is on average 25 degrees above "spring" is not hard.

Chuckles, as we also thought, is very high (top in fact) with 9,689 points of
prophetic wisdom. He began his illustrious career in only 2008 and has
successfully dominated the prophecy charts 15 times. The absolute legend
Punxsutawney Phil himself clocks in at only 5,569 widom points, proving that
there is still room for the young in the field of prophecy.

Now, let's look at a per state sum of prophetic wisdom.

>>>malloy
# bar_chart
run: RodentProphets -> {
  group_by: state
  aggregate: prophetic_wisdom is sum(prophetic_wisdom)
}

>>>markdown
Wow, Penselvania is the place to go for large-toothed prophecy!

Now let's look at the top prophecy states over time

>>>malloy
-- First let's get the top 5 states which we will call the "Prophecy5"
query: P5 is RodentProphets -> {
  group_by: state
  aggregate: sum_wisdom is sum(prophetic_wisdom)
  order_by: sum_wisdom desc
  top: 5
}
source: Prophecy5 is P5 extend { primary_key: state }

-- Then let's graph them over time
-- # line_chart
run: GroundhogPredictions -> {
  join_one: Prophecy5 with state
  group_by: day_year is `day`.year
  -- project: `day`, year_
  where: `day` < @2000
}


-- Then let's graph them over time
# line_chart
run: GroundhogPredictions -> {
  join_one: Prophecy5 with state
  where: Prophecy5.state != null
  group_by: `year` is `day`.year -- groundhog day
  aggregate: prophetic_wisdom is sum(prophetic_wisdom)
  group_by: state
  top: 1000 -- https://github.com/malloydata/malloy/issues/1291
}
>>>markdown
As we can see, Penselvania is still the small-furry-prophet center of the world
and also has the benefit of history when comparing total aggregated points, but
New York is an up-and-comming competitor!

>>>markdown
## Obligatory legal warning

![not a meterologist](https://www.digitalmomblog.com/wp-content/uploads/2022/01/Punxsutawney-Phil-groundhog-day-meme-.jpeg.webp)

I am neither a statistician nor a data scientist (I am, in fact, a programmer
with a love for programming) and many of these methods were made up by me
because they seemed the best I could do at the time.  If you know the _proper_
way to compare groundhog shadow-predictions please open an issue or a PR! If
it's not too much work, I will add an adendum :D

This code is Community Commons (CC0), but I strongly recommend against copying
either it or it's methology unless (perhaps) it's for your own journey to
learning Malloy and/or you would like to make fun of my work.

## Appendix
This is a worklog of the issues I hit and help I received while creating this
demo. Perhaps a Malloy developer will find it useful as a newbie experience
snapshot or something.

* Dockfixes to quickstart

  * `project *`  [pull request](https://github.com/malloydata/quickstart/pull/1) related to
    above: there is no schema explorer in the quickstart

  * quickstart bug [link](https://github.com/malloydata/quickstart/issues/2)

  * [Minor grammar pull request](https://github.com/malloydata/quickstart/pull/3)

* Documentation fixes

  * Grammar fixes in the documentation and some rewording suggestions
    ([link](https://github.com/malloydata/malloydata.github.io/pull/75)) Minor
    wording issue with namming a (percent tagged) a ratio as a percent

  * ([link](https://github.com/malloydata/malloydata.github.io/pull/81))

* Lanugage issues

  * ergonomics issue where you have to repeat yourself with `rename`/etc
    ([link](https://github.com/malloydata/malloy/issues/1288)) would be nice to

  * have assertions ([link](https://github.com/malloydata/malloy/issues/1290))

  * line chart requires `top` ([link](https://github.com/malloydata/malloy/issues/1291))

I also got help on
[slack](https://malloy-community.slack.com/archives/C025JAK8G0N/p1691817570702699)

Overall my expeience has been extremely positive.  It is a young language and
has a few sharp edges (which is to be expected) The maintainers were very
helpful and responded and merged the fixes I submitted promptly. It was
a very pleasant experience overall.

